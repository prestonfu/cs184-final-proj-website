<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8" />
  <meta name="description"
    content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos." />
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Real-Time Ray Tracing for Flocking Point Clouds</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"
    ></script> -->
  <script>
    MathJax = {
      tex: {
        inlineMath: [
          ["$", "$"],
          ["\\(", "\\)"],
        ],
      },
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag("js", new Date());

    gtag("config", "G-PYVRSFMDRL");
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|IBM+Plex+Sans|Noto+Sans|Castoro" rel="stylesheet" />

  <link rel="stylesheet" href="./static/css/bulma.min.css" />
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css" />
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css" />
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
  <link rel="stylesheet" href="./static/css/index.css" />
  <link rel="icon" href="./static/images/favicon.jpg" />

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>
  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>

    <!-- <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      
      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>
  </div> -->
  </nav>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              Real-Time Ray Tracing for Flocking Point Clouds
            </h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <p>Ralph Cao</p>
              </span>
              <span class="author-block">
                <p>3037721429</p>
              </span>
              <span class="author-block">
                <p>ralph_cow@</p>
              </span>
            </div>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <p>Arjun Damerla</p>
              </span>
              <span class="author-block">
                <p>3036679871</p>
              </span>
              <span class="author-block">
                <p>arjundamerla@</p>
              </span>
            </div>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <p>Preston Fu</p>
              </span>
              <span class="author-block">
                <p>3036364629</p>
              </span>
              <span class="author-block">
                <p>prestonfu@</p>
              </span>
            </div>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <p>Julia Sun</p>
              </span>
              <span class="author-block">
                <p>3037171474</p>
              </span>
              <span class="author-block">
                <p>jsun28@</p>
              </span>
            </div>
            <br />
            <div class="is-size-5 publication-authors">
              <span class="author-block">CS 184, Computer Graphics and Imaging</span>
            </div>

            <br />

            <!-- <div align="middle">
                <h2>
                  <a
                    href="https://docs.google.com/presentation/d/120M-5wzmt5IsNDmXZB47CACTrW2VtXqy05xqt6myKto/edit?usp=sharing"
                    >Slides Link</a
                  >
                </h2>
                <h2>
                  <a
                    href="https://drive.google.com/file/d/1ae08E61iPUZnDG8u-HIQmOJFnWpnZAq8/view"
                    >Video Link</a
                  >
                </h2>
              </div> -->

            <!-- 
          <div class="column has-text-centered">
            <div class="publication-links">
              <!--
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->

            <span class="link-block">
              <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fab fa-youtube"></i>
                </span>
                <span>Video</span>
              </a>
            </span>
            <!--
              <span class="link-block">https://prod.liveshare.vsengsaas.visualstudio.com/join?0A7051F5421BD8A68F474F037741D314857F
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              
              <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
          </div>
          -->
          <!-- </div> -->
        </div>
      </div>
    </div>
    </div>
  </section>

  <!--
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>

-->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title">TODO: insert demo video clip</h2>
        </div>
      </div>
    </div>

    <br />
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              In this project, we implemented real-time rendering using ray
              tracing and smoothened ray marching for large point clouds with
              camera movement. Furthermore, we implemented a pipeline using
              pre-trained machine learning models to generate 3D point clouds
              from text. In order to accelerate our rendering, we utilized a
              wide array of optimization techniques, including using GPU
              acceleration from the OpenCL framework, BVH trees, and Morton
              codes for Z-order space-filling curves. We also implemented a
              boid flocking algorithm with a force toward our pre-generated
              point clouds, which smoothly interpolates boid positions and
              colors. With 4096 boids, we achieve how many FPS with some more
              details, a somethingx speedup over ray tracing without
              acceleration.
            </p>

            <h2>TODO</h2>
          </div>
        </div>
      </div>

      <!-- Paper video. 
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    / Paper video. -->
    </div>

    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">Point Clouds</h2>
          <div class="content has-text-justified">
            <div align="middle">
              <img src="static/images/pointcloudpipeline.png" align="middle" width="1000px" />
            </div>
            <br />
            <p>
              Our approach is shown in the figure above. To collect point
              clouds, we:
            </p>
            <ol>
              <li>
                Generate text prompts with an LLM. We ran two prompts through
                Claude 3 independently to generate lists of prompts: one for
                real-life objects and well known characters for which I’d be
                able to find images on Google, and one for easily recognizable
                characters and objects performing whimsical actions.
              </li>
              <li>
                For the first list, we scraped images off Google — partially
                manually and partially with the
                <a href="https://serpapi.com/" target="_blank">Serp API</a>.
                For the second list, we ran prompts through
                <a href="https://openai.com/dall-e-3" target="_blank">Dall-E 3</a>. In each case, we specified that the
                image should have a
                white background.
              </li>
              <li>
                Preprocess the images, using a publicly available
                <a href="https://github.com/nadermx/backgroundremover" target="_blank">background remover</a>
                to remove image backgrounds and resizing and centering images.
              </li>
              <li>
                Input the resulting images to
                <a href="https://openai.com/research/point-e" target="_blank">Point-E</a>, which generates a 3D point
                cloud. We experimented with all
                combinations of {preprocessed, not preprocessed} and {300M,
                1B} model size, yielding a size-1024 point cloud $(x, y, z, r,
                g, b)$. Then we ran a pre-trained upsampler to yield a
                size-4096 point cloud of the same format. The model weights
                are publicly available, and we ran the scripts on one V100
                using Google Compute Engine. The total inference runtime for
                654 images was around 6 hours. (Our original hope in this
                project was that a user would enter text and have a point
                cloud within seconds, but this inference procedure is the
                bottleneck, in terms of time and cost.)

                <br />
                In general, we find that skipping the preprocessing step
                consistently results in noise or artifacts in the edges and
                corners of generated point clouds. We do not see a strong
                change in model performance between 300M and 1B parameters for
                contiguous objects, and that the common failure modes of
                extraneous black pixels and geometrically flat outputs appear
                consistently in both cases. For scenes containing multiple
                disjoint objects, we found that the 1B model generally
                performed better.
              </li>
            </ol>
            <p>
              The last step above resulted in point clouds with highly
              variable quality. We wrote a script to efficiently manually
              process these outputs. Out of around 150 randomly-sampled
              outputs, 40 were selected for use in our end product.
            </p>
          </div>
        </div>
      </div>
    </div>

    <div class="container is-max-desktop" style="margin-top: 2em">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">Point Clouds</h2>
          <div class="content has-text-justified">
            <div align="middle">
              <img src="static/images/flocking.png" align="middle" width="500px" />
            </div>
            <br />
            <p>
              Our flocking model incorporates the forces listed above. Each
              force has an associated radius, indicating that the force
              between two boids will only apply in the case that they are
              within some maximum $\ell_2$ distance. Each force also has a
              corresponding coefficient. These coefficients needed to be tuned
              quite carefully to achieve reasonable flocking behaviors: for
              instance, separation requires a specific value such that the
              boids do not suffer from mode collapse, and we introduced a weak
              “centering” force toward the origin to counteract the effect of
              “containing” trapping boids to the surface of a sphere. Our
              “arrival” force uses targets as the $(x,y,z)$ points generated
              in our point cloud procedure, which have been randomized by
              default to enable more cohesive transitions.
            </p>
          </div>
        </div>
      </div>
    </div>

    <div class="container is-max-desktop" style="margin-top: 2em">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">Flocking</h2>
          <div class="content has-text-justified">
            <div align="middle">
              <img src="static/images/flocking.png" align="middle" width="500px" />
            </div>
            <br />
            <p>
              Our flocking model incorporates the forces listed above. Each
              force has an associated radius, indicating that the force
              between two boids will only apply in the case that they are
              within some maximum $\ell_2$ distance. Each force also has a
              corresponding coefficient. These coefficients needed to be tuned
              quite carefully to achieve reasonable flocking behaviors: for
              instance, separation requires a specific value such that the
              boids do not suffer from mode collapse, and we introduced a weak
              “centering” force toward the origin to counteract the effect of
              “containing” trapping boids to the surface of a sphere. Our
              “arrival” force uses targets as the $(x,y,z)$ points generated
              in our point cloud procedure, which have been randomized by
              default to enable more cohesive transitions.
            </p>
          </div>
        </div>
      </div>
    </div>

    <div class="container is-max-desktop" style="margin-top: 2em">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">Ray Marching</h2>
          <div class="content has-text-justified">
            <p>Visualization of ray marching algorithm:</p>
            <div align="middle">
              <table style="width:100%">
                <tr align="center">
                  <td>
                    <img src="static/images/raymarching.png" align="middle" width="300px" />
                    <figcaption></figcaption>
                  </td>
                </tr>
              </table>
            </div>
            <p>Our ray marching algorithm was originally built on top of Homework 3 and tested in 
              that codebase. We also enabled changing camera positions and continuous imaging 
              renders. However, we found that the result was very laggy and rendered images slowly; 
              simply streaming camera inputs is insufficient to render the whole scene smoothly in 
              real time. Thus, we chose to accelerate this on the GPU using OpenCL, which we discuss 
              in the following section. </p>

            <p>The original approach is below.</p>

            <video width="560" height="480" controls>
              <source src="static/videos/raymarchingvid.mp4" type="video/mp4">
            </video>

            <p>We implemented boid smoothing, such that two nearby spheres can merge and thus smooth out 
              their collective surface. There are a wide variety 
              <a href="https://iquilezles.org/articles/smin/" target="_blank">wide variety</a> of available 
              smooth-min functions. We implemented </p>

            <h2>TODO: add close-up image of smoothing, and include the required parameters to generate that image</h2>

            <p>Let $d_1$ denote the distance from the camera to the first sphere and $d_2$ to the second sphere. 
              We also let $b = 10^{-3}$ be a buffer constant. Then ray marching uses a distance defined by
              $$\min(d_1, d_2) - \frac b6 \left(\frac{\max(b - |d_1 - d_2|, 0)}{b}\right)^3$$</p>
            
          </div>
        </div>
      </div>
    </div>

  <div class="container is-max-desktop" style="margin-top: 2em">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Acceleration structures</h2>
        <div class="content has-text-justified">
          <h4>Motorn Sort</h4>
          <ul>
            <li>In order to implement smoothing for our ray marching algorithm, we had to figure out which 
              spheres were very close to each other, so that we could effectively mesh the two objects into 
              one, as seen in the picture below.</li> 
  
            <li>Normally, figuring out intersections is an O(n^2) operation. However, in order to further 
              optimize this intersection test, we decided to implement a spatial sort that preserves locality 
              through the use of Morton Codes. Morton codes effectively help sort our spheres in only ONE 
              dimension, by squashing down all our sphere positions into a single number, and then comparing 
              these numbers with each other. Since this sorting preserves spatial locality, if two sphere 
              positions are close to each other, then the two morton code values for each sphere will also 
              be very close to each other.</li> 
  
            <li>At a high level, a morton code for some spatial position (p_1, p_2, … , p_n) will find the binary
              representation for each p_i, and then interleave these binary values together, resulting in a single, 
              large binary value b_1. Miraculously, if two positions (p_1, p_2, … , p_n) and (q_1, q_2, …, q_n) are 
              very close to each other, then their corresponding morton codes, b_1 and b_2, will have values that 
              are very close to each other. </li> 
  
            <li>This results in figuring out the distances between boids to be much more efficient, as we are only 
              comparing single values now to see how close these spheres are from each other.</li>
          </ul>
          
        </div>
      </div>
    </div>
  </div>

  <div class="container is-max-desktop" style="margin-top: 2em">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Open CL</h2>
        <div class="content has-text-justified">
          <p>We ran into a number of challenges when using OpenCL. None of us had any GPU experience on this level 
            or any experience with OpenCL, so we had to learn the OpenCL environment from scratch, and had to navigate 
            through GPU specific concepts such as work group thread synchronization through barrier calls, command 
            queues, and GPU buffers.</p> <br>

            <h4>Deprecation</h4>
            <ul>
            <li>Apple dropped support for OpenCL in favor of their proprietary Metal API, leading to some incredibly 
              difficult to trace bugs. After 4 hours of debugging on our first draft of the ray tracing compute 
              shader, we realized that functions with pointer arguments had unpredictable behavior, and we were
              able to proceed with our implementation by limiting the use of helper functions.</li>
            </ul>

            <h4>Speed</h4>
            <ul>
            <li>Despite reducing the number of operations by an order of magnitude, we still needed a number of 
              optimizations in order to increase the frame rate to an acceptable amount for a stable simulation 
              and comfortable user experience. These optimizations include utilizing local memory and work groups 
              on the GPU, choosing an optimal work group size, and using an acceleration structure. These 
              optimizations are benchmarked in the tables in the Results section.</li>

            <li>Our work group optimizations were generally based on finding a spatial sorting of the points using 
              a permutation index array that grouped together points that were local and minimized the bounding 
              boxes for each.</li>
            </ul> 
        </div>
      </div>
    </div>
  </div>
  </section>

  <div class="container is-max-desktop" style="margin-top: 2em">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Application Development</h2>
        <div class="content has-text-justified">
          <p>We started from a sample project using OpenCL and OpenGL interoperations to render fractals to a window.
            This project was limited to interaction through the number keys to select different fractals, and used an 
            OpenCL compute shader to generate an image that was passed to OpenGL to render to a window.</p>
          <p>On top of this, we built ray tracing and ray marching compute shaders, particle simulation and integration 
            compute shaders, camera movement through click and drag, as well as keyboard inputs to switch between different
            point clouds. We also integrated both the ray marching and ray tracing with light blocking implementations 
            into one application where the user is freely able to switch between visuals.</p>
        </div>
      </div>
    </div>
  </div>

  <div class="container is-max-desktop" style="margin-top: 2em">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Profiling</h2>
        <div class="content has-text-justified">
          
        </div>
      </div>
    </div>
  </div>

  <div class="container is-max-desktop" style="margin-top: 2em">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Results</h2>
        <div class="content has-text-justified">
          
        </div>
      </div>
    </div>
  </div>
  


  <div class="container is-max-desktop" style="margin-top: 2em">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">References</h2>
        <div class="content has-text-justified">
          <p>External resources were useful for the following:</p>
          <ul>
            <li>
              <a href="https://jamie-wong.com/2016/07/15/ray-marching-signed-distance-functions/#the-raymarching-algorithm"
                target="_blank">Ray Marching algorithm and smoothing</a>
            </li>
            <li><a href="https://iquilezles.org/articles/smin/"" target=" _blank">Smooth minimum SDF</a></li>
            <li>
              <a href="https://developer.nvidia.com/blog/thinking-parallel-part-ii-tree-traversal-gpu/"
                target="_blank">BVH on GPU</a>
            </li>
            <li>
              <a href="https://cs184.eecs.berkeley.edu/sp24/lecture/9/ray-tracing-and-acceleration-str"
                target="_blank">Ray intersection with axis aligned box</a>
            </li>
            <li>
              <a href="https://github.com/9prady9/CLGLInterop" target="_blank">OpenCL + OpenGL Interop</a>
            </li>
            <li>
              Camera code from Homework 3
            </li>
          </ul>
        </div>
      </div>
    </div>
  </div>
  </section>

  <!-- 
<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      Visual Effects. 
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Visual Effects</h2>
          <p>
            Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
            would be impossible without nerfies since it would require going through a wall.
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/dollyzoom-stacked.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
     

     
      <div class="column">
        <h2 class="title is-3">Matting</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              As a byproduct of our method, we can also solve the matting problem by ignoring
              samples that fall outside of a bounding box during rendering.
            </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/matting.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div>
    / Matting. -->

  <!-- Animation. 
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Animation</h2>-->

  <!-- Interpolating. 
        <h3 class="title is-4">Interpolating states</h3>
        <div class="content has-text-justified">
          <p>
            We can also animate the scene by interpolating the deformation latent codes of two input
            frames. Use the slider here to linearly interpolate between the left frame and the right
            frame.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_start.jpg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Start Frame</p>
          </div>
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div>
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div>
        </div>
        <br/>
        / Interpolating. -->

  <!-- Re-rendering. 
        <h3 class="title is-4">Re-rendering the input video</h3>
        <div class="content has-text-justified">
          <p>
            Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
            viewpoint such as a stabilized camera by playing back the training deformations.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/replay.mp4"
                    type="video/mp4">
          </video>
        </div>
        Re-rendering. 

      </div>
    </div>
      -->

  <!--Concurrent Work. 
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div> -->
  <!--/ Concurrent Work. 

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section>
-->

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content" align="middle">
            The website design is adapted from
            <a href="https://github.com/nerfies/nerfies.github.io">here</a>.
          </div>
        </div>
      </div>
    </div>
  </footer>
</body>

</html>